{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDkHjj6sXHY2"
      },
      "source": [
        "# MGMT 467 — Prompt-Driven Lab (with Commented Examples)\n",
        "## Kaggle ➜ Google Cloud Storage ➜ BigQuery ➜ Data Quality (DQ)\n",
        "\n",
        "**How to use this notebook**\n",
        "- Each section gives you a **Build Prompt** to paste into Gemini/Vertex AI (or Gemini in Colab).\n",
        "- Below each prompt, you’ll see a **commented example** of what a good LLM answer might look like.\n",
        "- **Do not** just uncomment and run. Use the prompt to generate your own code, then compare to the example.\n",
        "- After every step, run the **Verification Prompt**, and write the **Reflection** in Markdown.\n",
        "\n",
        "> Goal today: Download the Netflix dataset (Kaggle) → Stage on GCS → Load into BigQuery → Run DQ profiling (missingness, duplicates, outliers, anomaly flags).\n"
      ],
      "id": "WDkHjj6sXHY2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwfXwduPXHY5"
      },
      "source": [
        "### Academic integrity & LLM usage\n",
        "- Use the prompts here to generate your own code cells.\n",
        "- Read concept notes and write the reflection answers in your own words.\n",
        "- Keep credentials out of code. Upload `kaggle.json` when asked.\n"
      ],
      "id": "nwfXwduPXHY5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRkrN9LaXHY6"
      },
      "source": [
        "## Learning objectives\n",
        "1) Explain **why** we stage data in GCS and load it to BigQuery.  \n",
        "2) Build an **idempotent**, auditable pipeline.  \n",
        "3) Diagnose **missingness**, **duplicates**, and **outliers** and justify cleaning choices.  \n",
        "4) Connect DQ decisions to **business/ML impact**.\n"
      ],
      "id": "iRkrN9LaXHY6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "payWUkvGXHY6"
      },
      "source": [
        "## 0) Environment setup — What & Why\n",
        "Authenticate Colab to Google Cloud so we can use `gcloud`, GCS, and BigQuery. Set **PROJECT_ID** and **REGION** once for consistency (cost/latency)."
      ],
      "id": "payWUkvGXHY6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kvPwlDeXHY7"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "2kvPwlDeXHY7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpjlimJVXHY7"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Auth + Project/Region (commented; write your own cell using the prompt)\n",
        "# # from google.colab import auth\n",
        "# # auth.authenticate_user()\n",
        "# #\n",
        "# # import os\n",
        "# # PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "# # REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "# # os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "# # print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "# #\n",
        "# # # Set active project for gcloud/BigQuery CLI\n",
        "# # !gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "# # !gcloud config get-value project\n",
        "# # # Done: Auth + Project/Region set"
      ],
      "id": "cpjlimJVXHY7"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "PROJECT_ID = input(\"Enter your GCP Project ID: \").strip()\n",
        "REGION = \"us-central1\"  # keep consistent; change if instructed\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "print(\"Project:\", PROJECT_ID, \"| Region:\", REGION)\n",
        "\n",
        "# Set active project for gcloud/BigQuery CLI\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "!gcloud config get-value project\n",
        "# Done: Auth + Project/Region set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsRaDKjwZptk",
        "outputId": "90291aef-93c4-4c13-86d6-c2802aac0067"
      },
      "id": "lsRaDKjwZptk",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your GCP Project ID: mgmt-467-471819\n",
            "Project: mgmt-467-471819 | Region: us-central1\n",
            "Updated property [core/project].\n",
            "mgmt-467-471819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7rRF40NXHY8"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a short cell that prints the active project using `gcloud config get-value project` and echoes the `REGION` you set.\n"
      ],
      "id": "C7rRF40NXHY8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify configuration using gcloud and local variables\n",
        "\n",
        "# Execute gcloud command to get the currently set project\n",
        "print(\"Active Project ID:\")\n",
        "!gcloud config get-value project\n",
        "\n",
        "# Print the Python variable REGION (assuming it was defined in the previous cell)\n",
        "print(\"\\nConfigured Region:\")\n",
        "print(REGION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIPV2VcJaZL7",
        "outputId": "1d99e187-48a0-4758-be32-c80085de68f2"
      },
      "id": "sIPV2VcJaZL7",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Active Project ID:\n",
            "mgmt-467-471819\n",
            "\n",
            "Configured Region:\n",
            "us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fptWY8mtXHY8"
      },
      "source": [
        "**Reflection:** Why do we set `PROJECT_ID` and `REGION` at the top? What can go wrong if we don’t?"
      ],
      "id": "fptWY8mtXHY8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "REFLECTION: So that it can be easily referenced across the rest and can be used for multiple different projects."
      ],
      "metadata": {
        "id": "lAP0fPIXarJ7"
      },
      "id": "lAP0fPIXarJ7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwhcNi68XHY8"
      },
      "source": [
        "## 1) Kaggle API — What & Why\n",
        "Use Kaggle CLI for reproducible downloads. Store `kaggle.json` at `~/.kaggle/kaggle.json` with `0600` permissions to protect secrets."
      ],
      "id": "TwhcNi68XHY8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQKzWBhTXHY8"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **single Colab code cell** that:\n",
        "- Prompts me to upload `kaggle.json`,\n",
        "- Saves to `~/.kaggle/kaggle.json` with `0600` permissions,\n",
        "- Prints `kaggle --version`.\n",
        "Add comments about security and reproducibility.\n"
      ],
      "id": "GQKzWBhTXHY8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhfmEOF2XHY9"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Kaggle setup (commented)\n",
        "# # from google.colab import files\n",
        "# # print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "# # uploaded = files.upload()\n",
        "# #\n",
        "# # import os\n",
        "# # os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "# # with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "# #     f.write(uploaded[list(uploaded.keys())[0]])\n",
        "# # os.chmod('/root/.kaggle/kaggle.json', 0o600)  # owner-only\n",
        "# #\n",
        "# # !kaggle --version"
      ],
      "id": "IhfmEOF2XHY9"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "print(\"Upload your kaggle.json (Kaggle > Account > Create New API Token)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "with open('/root/.kaggle/kaggle.json', 'wb') as f:\n",
        "    f.write(uploaded[list(uploaded.keys())[0]])\n",
        "os.chmod('/root/.kaggle/kaggle.json', 0o600)  # owner-only\n",
        "\n",
        "!kaggle --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "8HNc7Wvpbe74",
        "outputId": "f797ced3-8725-47e8-cd7e-e1df70a784ff"
      },
      "id": "8HNc7Wvpbe74",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your kaggle.json (Kaggle > Account > Create New API Token)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-20ac15a2-9df3-42d1-8036-c78065f7c3e9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-20ac15a2-9df3-42d1-8036-c78065f7c3e9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNBTp_GhXHY9"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a one-liner that runs `kaggle --help | head -n 20` to show the CLI is ready.\n"
      ],
      "id": "KNBTp_GhXHY9"
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle --help | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSC3-CLEdxxR",
        "outputId": "f374eb20-67ea-4b5e-eedd-272726396797"
      },
      "id": "JSC3-CLEdxxR",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: kaggle [-h] [-v] [-W]\n",
            "              {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "              ...\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -v, --version         Print the Kaggle API version\n",
            "  -W, --no-warn         Disable out-of-date API version warning\n",
            "\n",
            "commands:\n",
            "  {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "                        Use one of:\n",
            "                        competitions {list, files, download, submit, submissions, leaderboard}\n",
            "                        datasets {list, files, download, create, version, init, metadata, status}\n",
            "                        kernels {list, files, init, push, pull, output, status}\n",
            "                        models {instances, get, list, init, create, delete, update}\n",
            "                        models instances {versions, get, files, init, create, delete, update}\n",
            "                        models instances versions {init, create, download, delete, files}\n",
            "                        config {view, set, unset}\n",
            "    competitions (c)    Commands related to Kaggle competitions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiL839gsXHY9"
      },
      "source": [
        "**Reflection:** Why require strict `0600` permissions on API tokens? What risks are we avoiding? It helps keep everything secure."
      ],
      "id": "xiL839gsXHY9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHgD-ZCFXHY9"
      },
      "source": [
        "## 2) Download & unzip dataset — What & Why\n",
        "Keep raw files under `/content/data/raw` for predictable paths and auditing.\n",
        "**Dataset:** `sayeeduddin/netflix-2025user-behavior-dataset-210k-records`"
      ],
      "id": "yHgD-ZCFXHY9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBxZxJxyXHY9"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates `/content/data/raw`,\n",
        "- Downloads the dataset to `/content/data` with Kaggle CLI,\n",
        "- Unzips into `/content/data/raw` (overwrite OK),\n",
        "- Lists all CSVs with sizes in a neat table.\n",
        "Include comments describing each step.\n"
      ],
      "id": "CBxZxJxyXHY9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjHZH2lvXHY-"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Download & unzip (commented)\n",
        "# # !mkdir -p /content/data/raw\n",
        "# # !kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "# # !unzip -o /content/data/*.zip -d /content/data/raw\n",
        "# # # List CSV inventory\n",
        "# # !ls -lh /content/data/raw/*.csv"
      ],
      "id": "VjHZH2lvXHY-"
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data/raw\n",
        "!kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "!unzip -o /content/data/*.zip -d /content/data/raw\n",
        "# List CSV inventory\n",
        "!ls -lh /content/data/raw/*.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ1bXKyfd-5A",
        "outputId": "d6ae3cfd-39ad-4949-f7bd-bd80ac1b9df8"
      },
      "id": "RJ1bXKyfd-5A",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sayeeduddin/netflix-2025user-behavior-dataset-210k-records\n",
            "License(s): CC0-1.0\n",
            "Downloading netflix-2025user-behavior-dataset-210k-records.zip to /content/data\n",
            "  0% 0.00/4.02M [00:00<?, ?B/s]\n",
            "100% 4.02M/4.02M [00:00<00:00, 631MB/s]\n",
            "Archive:  /content/data/netflix-2025user-behavior-dataset-210k-records.zip\n",
            "  inflating: /content/data/raw/README.md  \n",
            "  inflating: /content/data/raw/movies.csv  \n",
            "  inflating: /content/data/raw/recommendation_logs.csv  \n",
            "  inflating: /content/data/raw/reviews.csv  \n",
            "  inflating: /content/data/raw/search_logs.csv  \n",
            "  inflating: /content/data/raw/users.csv  \n",
            "  inflating: /content/data/raw/watch_history.csv  \n",
            "-rw-r--r-- 1 root root 114K Aug  2 19:36 /content/data/raw/movies.csv\n",
            "-rw-r--r-- 1 root root 4.5M Aug  2 19:36 /content/data/raw/recommendation_logs.csv\n",
            "-rw-r--r-- 1 root root 1.8M Aug  2 19:36 /content/data/raw/reviews.csv\n",
            "-rw-r--r-- 1 root root 2.2M Aug  2 19:36 /content/data/raw/search_logs.csv\n",
            "-rw-r--r-- 1 root root 1.6M Aug  2 19:36 /content/data/raw/users.csv\n",
            "-rw-r--r-- 1 root root 8.9M Aug  2 19:36 /content/data/raw/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3OkIq9SXHY-"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that asserts there are exactly **six** CSV files and prints their names.\n"
      ],
      "id": "K3OkIq9SXHY-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85128f5b",
        "outputId": "29f368cb-339d-4635-aa96-925ac8e9dd53"
      },
      "source": [
        "import glob\n",
        "\n",
        "csv_files = glob.glob('/content/data/raw/*.csv')\n",
        "num_csv_files = len(csv_files)\n",
        "\n",
        "assert num_csv_files == 6, f\"Expected 6 CSV files, but found {num_csv_files}\"\n",
        "\n",
        "print(\"Found exactly 6 CSV files:\")\n",
        "for csv_file in csv_files:\n",
        "    print(csv_file)"
      ],
      "id": "85128f5b",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found exactly 6 CSV files:\n",
            "/content/data/raw/movies.csv\n",
            "/content/data/raw/watch_history.csv\n",
            "/content/data/raw/search_logs.csv\n",
            "/content/data/raw/users.csv\n",
            "/content/data/raw/recommendation_logs.csv\n",
            "/content/data/raw/reviews.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rIDUwHrXHY-"
      },
      "source": [
        "**Reflection:** Why is keeping a clean file inventory (names, sizes) useful downstream? It makes it easy to run queries fast so that it is not running for useless points."
      ],
      "id": "6rIDUwHrXHY-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hicdY2NXHY-"
      },
      "source": [
        "## 3) Create GCS bucket & upload — What & Why\n",
        "Stage in GCS → consistent, versionable source for BigQuery loads. Bucket names must be **globally unique**."
      ],
      "id": "1hicdY2NXHY-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw-mFowxXHY-"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates a unique bucket in `${REGION}` (random suffix),\n",
        "- Saves name to `BUCKET_NAME` env var,\n",
        "- Uploads all CSVs to `gs://$BUCKET_NAME/netflix/`,\n",
        "- Prints the bucket name and explains staging benefits.\n"
      ],
      "id": "Pw-mFowxXHY-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2U_wUZMXHY-"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — GCS staging (commented)\n",
        "# # import uuid, os\n",
        "# # bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "# # os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "# # !gcloud storage buckets create gs://$BUCKET_NAME --location=$REGION\n",
        "# # !gcloud storage cp /content/data/raw/* gs://$BUCKET_NAME/netflix/\n",
        "# # print(\"Bucket:\", bucket_name)\n",
        "# # # Verify contents\n",
        "# # !gcloud storage ls gs://$BUCKET_NAME/netflix/"
      ],
      "id": "C2U_wUZMXHY-"
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid, os\n",
        "bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "# Use 'US' for multi-region bucket location\n",
        "!gcloud storage buckets create gs://$BUCKET_NAME --location=US\n",
        "!gcloud storage cp /content/data/raw/* gs://$BUCKET_NAME/netflix/\n",
        "print(\"Bucket:\", bucket_name)\n",
        "# Verify contents\n",
        "!gcloud storage ls gs://$BUCKET_NAME/netflix/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guQigIMke62x",
        "outputId": "784a8db1-817d-4100-e766-19137fff0e9b"
      },
      "id": "guQigIMke62x",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://mgmt467-netflix-a38435f3/...\n",
            "Copying file:///content/data/raw/movies.csv to gs://mgmt467-netflix-a38435f3/netflix/movies.csv\n",
            "Copying file:///content/data/raw/README.md to gs://mgmt467-netflix-a38435f3/netflix/README.md\n",
            "Copying file:///content/data/raw/recommendation_logs.csv to gs://mgmt467-netflix-a38435f3/netflix/recommendation_logs.csv\n",
            "Copying file:///content/data/raw/reviews.csv to gs://mgmt467-netflix-a38435f3/netflix/reviews.csv\n",
            "Copying file:///content/data/raw/search_logs.csv to gs://mgmt467-netflix-a38435f3/netflix/search_logs.csv\n",
            "Copying file:///content/data/raw/users.csv to gs://mgmt467-netflix-a38435f3/netflix/users.csv\n",
            "Copying file:///content/data/raw/watch_history.csv to gs://mgmt467-netflix-a38435f3/netflix/watch_history.csv\n",
            "\n",
            "Average throughput: 46.8MiB/s\n",
            "Bucket: mgmt467-netflix-a38435f3\n",
            "gs://mgmt467-netflix-a38435f3/netflix/README.md\n",
            "gs://mgmt467-netflix-a38435f3/netflix/movies.csv\n",
            "gs://mgmt467-netflix-a38435f3/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-netflix-a38435f3/netflix/reviews.csv\n",
            "gs://mgmt467-netflix-a38435f3/netflix/search_logs.csv\n",
            "gs://mgmt467-netflix-a38435f3/netflix/users.csv\n",
            "gs://mgmt467-netflix-a38435f3/netflix/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHfq1-5rXHY-"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that lists the `netflix/` prefix and shows object sizes.\n"
      ],
      "id": "zHfq1-5rXHY-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af0e8ee8",
        "outputId": "1c2851dd-100a-40f2-9199-070f136339ec"
      },
      "source": [
        "import os\n",
        "# Verify the contents of the 'netflix/' prefix and show sizes\n",
        "!gcloud storage ls -l gs://{os.environ['BUCKET_NAME']}/netflix/"
      ],
      "id": "af0e8ee8",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      8002  2025-10-26T22:03:49Z  gs://mgmt467-netflix-a38435f3/netflix/README.md\n",
            "    115942  2025-10-26T22:03:49Z  gs://mgmt467-netflix-a38435f3/netflix/movies.csv\n",
            "   4695557  2025-10-26T22:03:49Z  gs://mgmt467-netflix-a38435f3/netflix/recommendation_logs.csv\n",
            "   1861942  2025-10-26T22:03:49Z  gs://mgmt467-netflix-a38435f3/netflix/reviews.csv\n",
            "   2250902  2025-10-26T22:03:49Z  gs://mgmt467-netflix-a38435f3/netflix/search_logs.csv\n",
            "   1606820  2025-10-26T22:03:49Z  gs://mgmt467-netflix-a38435f3/netflix/users.csv\n",
            "   9269425  2025-10-26T22:03:49Z  gs://mgmt467-netflix-a38435f3/netflix/watch_history.csv\n",
            "TOTAL: 7 objects, 19808590 bytes (18.89MiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_aUvI2iXHY_"
      },
      "source": [
        "**Reflection:** Name two benefits of staging in GCS vs loading directly from local Colab. It keeps it from being have to be reloaded in here repeatedly."
      ],
      "id": "I_aUvI2iXHY_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEvmPunbXHY_"
      },
      "source": [
        "## 4) BigQuery dataset & loads — What & Why\n",
        "Create dataset `netflix` and load six CSVs with **autodetect** for speed (we’ll enforce schemas later)."
      ],
      "id": "yEvmPunbXHY_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6MshiC4XHY_"
      },
      "source": [
        "### Build Prompt (two cells)\n",
        "**Cell A:** Create (idempotently) dataset `netflix` in US multi-region; if it exists, print a friendly message.  \n",
        "**Cell B:** Load tables from `gs://$BUCKET_NAME/netflix/`:\n",
        "`users, movies, watch_history, recommendation_logs, search_logs, reviews`\n",
        "with `--skip_leading_rows=1 --autodetect --source_format=CSV`.\n",
        "Finish with row-count queries for each table.\n"
      ],
      "id": "a6MshiC4XHY_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtPlSeNVXHY_"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — BigQuery dataset (commented)\n",
        "# # DATASET=\"netflix\"\n",
        "# # # Attempt to create; ignore if exists\n",
        "# # !bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "CtPlSeNVXHY_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GWc3-DrXHY_"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Load tables (commented)\n",
        "# # tables = {\n",
        "# #   \"users\": \"users.csv\",\n",
        "# #   \"movies\": \"movies.csv\",\n",
        "# #   \"watch_history\": \"watch_history.csv\",\n",
        "# #   \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "# #   \"search_logs\": \"search_logs.csv\",\n",
        "# #   \"reviews\": \"reviews.csv\",\n",
        "# # }\n",
        "# # import os\n",
        "# # for tbl, fname in tables.items():\n",
        "# #   src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "# #   print(\"Loading\", tbl, \"from\", src)\n",
        "# #   !bq load --skip_leading_rows=1 --autodetect --source_format=CSV $DATASET.$tbl $src\n",
        "# #\n",
        "# # # Row counts\n",
        "# # for tbl in tables.keys():\n",
        "# #   !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM `${GOOGLE_CLOUD_PROJECT}.netflix.{tbl}`\".format(tbl=tbl)"
      ],
      "id": "6GWc3-DrXHY_"
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET=\"netflix\"\n",
        "# Attempt to create; ignore if exists\n",
        "!bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLrg-cUij8oa",
        "outputId": "93d91b24-e778-4015-b767-c11ed8c88e12"
      },
      "id": "qLrg-cUij8oa",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BigQuery error in mk operation: Dataset 'mgmt-467-471819:netflix' already\n",
            "exists.\n",
            "Dataset may already exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tables = {\n",
        "  \"users\": \"users.csv\",\n",
        "  \"movies\": \"movies.csv\",\n",
        "  \"watch_history\": \"watch_history.csv\",\n",
        "  \"recommendation_logs\": \"recommendation_logs.csv\",\n",
        "  \"search_logs\": \"search_logs.csv\",\n",
        "  \"reviews\": \"reviews.csv\",\n",
        "}\n",
        "import os\n",
        "DATASET = \"netflix\" # Ensure DATASET is defined in this cell or accessible\n",
        "\n",
        "for tbl, fname in tables.items():\n",
        "  src = f\"gs://{os.environ['BUCKET_NAME']}/netflix/{fname}\"\n",
        "  print(\"Loading\", tbl, \"from\", src)\n",
        "  # Corrected bq load syntax: destination_table comes before source\n",
        "  !bq load --skip_leading_rows=1 --autodetect --source_format=CSV {DATASET}.{tbl} {src}\n",
        "\n",
        "# Row counts\n",
        "for tbl in tables.keys():\n",
        "  # Corrected bq query syntax to escape backticks for shell execution\n",
        "  print(f\"Getting row count for {tbl}\")\n",
        "  !bq query --nouse_legacy_sql \"SELECT '{tbl}' AS table_name, COUNT(*) AS n FROM \\`{os.environ['GOOGLE_CLOUD_PROJECT']}.{DATASET}.{tbl}\\`\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM89EaCpkGAK",
        "outputId": "08431521-29ae-48d1-c58e-209f92f07d64"
      },
      "id": "oM89EaCpkGAK",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading users from gs://mgmt467-netflix-a38435f3/netflix/users.csv\n",
            "Waiting on bqjob_r2441932a136d22f0_0000019a228d0038_1 ... (1s) Current status: DONE   \n",
            "Loading movies from gs://mgmt467-netflix-a38435f3/netflix/movies.csv\n",
            "Waiting on bqjob_r3d8e9aa1dc29e2ea_0000019a228d1687_1 ... (1s) Current status: DONE   \n",
            "Loading watch_history from gs://mgmt467-netflix-a38435f3/netflix/watch_history.csv\n",
            "Waiting on bqjob_r5b5d0a2e0bcecc5d_0000019a228d2c23_1 ... (2s) Current status: DONE   \n",
            "Loading recommendation_logs from gs://mgmt467-netflix-a38435f3/netflix/recommendation_logs.csv\n",
            "Waiting on bqjob_r49516a7a55608b96_0000019a228d4569_1 ... (1s) Current status: DONE   \n",
            "Loading search_logs from gs://mgmt467-netflix-a38435f3/netflix/search_logs.csv\n",
            "Waiting on bqjob_r6dabd7b0eb74ceb0_0000019a228d5bbf_1 ... (1s) Current status: DONE   \n",
            "Loading reviews from gs://mgmt467-netflix-a38435f3/netflix/reviews.csv\n",
            "Waiting on bqjob_r3e42044d9e1a596f_0000019a228d70f1_1 ... (1s) Current status: DONE   \n",
            "Getting row count for users\n",
            "+------------+-------+\n",
            "| table_name |   n   |\n",
            "+------------+-------+\n",
            "| users      | 41200 |\n",
            "+------------+-------+\n",
            "Getting row count for movies\n",
            "+------------+------+\n",
            "| table_name |  n   |\n",
            "+------------+------+\n",
            "| movies     | 4160 |\n",
            "+------------+------+\n",
            "Getting row count for watch_history\n",
            "+---------------+--------+\n",
            "|  table_name   |   n    |\n",
            "+---------------+--------+\n",
            "| watch_history | 420000 |\n",
            "+---------------+--------+\n",
            "Getting row count for recommendation_logs\n",
            "+---------------------+--------+\n",
            "|     table_name      |   n    |\n",
            "+---------------------+--------+\n",
            "| recommendation_logs | 208000 |\n",
            "+---------------------+--------+\n",
            "Getting row count for search_logs\n",
            "+-------------+--------+\n",
            "| table_name  |   n    |\n",
            "+-------------+--------+\n",
            "| search_logs | 106000 |\n",
            "+-------------+--------+\n",
            "Getting row count for reviews\n",
            "+------------+-------+\n",
            "| table_name |   n   |\n",
            "+------------+-------+\n",
            "| reviews    | 61800 |\n",
            "+------------+-------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihy1_7M5XHY_"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single query that returns `table_name, row_count` for all six tables in `${GOOGLE_CLOUD_PROJECT}.netflix`.\n"
      ],
      "id": "Ihy1_7M5XHY_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "493e96c4",
        "outputId": "88cd8344-09ff-4df7-90ee-b19f085004c8"
      },
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client()\n",
        "\n",
        "# Compose unified query\n",
        "query = f\"\"\"\n",
        "SELECT 'users' AS table_name, COUNT(*) AS row_count FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.users`\n",
        "UNION ALL\n",
        "SELECT 'movies' AS table_name, COUNT(*) AS row_count FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.movies`\n",
        "UNION ALL\n",
        "SELECT 'watch_history' AS table_name, COUNT(*) AS row_count FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT 'recommendation_logs' AS table_name, COUNT(*) AS row_count FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.recommendation_logs`\n",
        "UNION ALL\n",
        "SELECT 'search_logs' AS table_name, COUNT(*) AS row_count FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.search_logs`\n",
        "UNION ALL\n",
        "SELECT 'reviews' AS table_name, COUNT(*) AS row_count FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.reviews`\n",
        "\"\"\"\n",
        "\n",
        "# Run query and convert results to DataFrame\n",
        "results = client.query(query).to_dataframe()\n",
        "\n",
        "# Display results\n",
        "print(results)"
      ],
      "id": "493e96c4",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            table_name  row_count\n",
            "0              reviews      61800\n",
            "1               movies       4160\n",
            "2        watch_history     420000\n",
            "3                users      41200\n",
            "4  recommendation_logs     208000\n",
            "5          search_logs     106000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yo-dVAPXHY_"
      },
      "source": [
        "**Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why? Autodetect is good when you do not know the names of the tables, but if you do know it then you should not use it."
      ],
      "id": "0Yo-dVAPXHY_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqg1Jb1IXHZA"
      },
      "source": [
        "## 5) Data Quality (DQ) — Concepts we care about\n",
        "- **Missingness** (MCAR/MAR/MNAR). Impute vs drop. Add `is_missing_*` indicators.\n",
        "- **Duplicates** (exact vs near). Double-counted engagement corrupts labels & KPIs.\n",
        "- **Outliers** (IQR). Winsorize/cap vs robust models. Always **flag** and explain.\n",
        "- **Reproducibility**. Prefer `CREATE OR REPLACE` and deterministic keys.\n"
      ],
      "id": "pqg1Jb1IXHZA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh6ORJ4iXHZA"
      },
      "source": [
        "### 5.1 Missingness (users) — What & Why\n",
        "Measure % missing and check if missingness depends on another variable (MAR) → potential bias & instability."
      ],
      "id": "Wh6ORJ4iXHZA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skCQ-OTuXHZA"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Total rows and % missing in `region`, `plan_tier`, `age_band` from `users`.\n",
        "2) `% plan_tier missing by region` ordered descending. Add comments on MAR.\n"
      ],
      "id": "skCQ-OTuXHZA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DM_ocnYoXHZA"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Missingness profile (commented)\n",
        "# # -- Users: % missing per column\n",
        "# # WITH base AS (\n",
        "# #   SELECT COUNT(*) n,\n",
        "# #          COUNTIF(region IS NULL) miss_region,\n",
        "# #          COUNTIF(plan_tier IS NULL) miss_plan,\n",
        "# #          COUNTIF(age_band IS NULL) miss_age\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`\n",
        "# # )\n",
        "# # SELECT n,\n",
        "# #        ROUND(100*miss_region/n,2) AS pct_missing_region,\n",
        "# #        ROUND(100*miss_plan/n,2)   AS pct_missing_plan_tier,\n",
        "# #        ROUND(100*miss_age/n,2)    AS pct_missing_age_band\n",
        "# # FROM base;"
      ],
      "id": "DM_ocnYoXHZA"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client()\n",
        "\n",
        "query1 = f\"\"\"\n",
        "WITH base AS (\n",
        "  SELECT\n",
        "    COUNT(*) AS n,\n",
        "    COUNTIF(country IS NULL) AS miss_country,\n",
        "    COUNTIF(subscription_plan IS NULL) AS miss_plan,\n",
        "    COUNTIF(age IS NULL) AS miss_age\n",
        "  FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.users`\n",
        ")\n",
        "SELECT\n",
        "  n,\n",
        "  ROUND(100 * miss_country / n, 2) AS pct_missing_country,\n",
        "  ROUND(100 * miss_plan / n, 2) AS pct_missing_subscription_plan,\n",
        "  ROUND(100 * miss_age / n, 2) AS pct_missing_age\n",
        "FROM base\n",
        "\"\"\"\n",
        "\n",
        "missing_summary = client.query(query1).to_dataframe()\n",
        "print(\"Total rows and % missing per column:\")\n",
        "print(missing_summary)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMVEVNMonvMW",
        "outputId": "905e25c3-7431-4050-d07f-6a8e01374ec8"
      },
      "id": "YMVEVNMonvMW",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows and % missing per column:\n",
            "       n  pct_missing_country  pct_missing_subscription_plan  pct_missing_age\n",
            "0  41200                  0.0                            0.0            11.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUv-mDNvXHZA"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — MAR by region (commented)\n",
        "# # SELECT region,\n",
        "# #        COUNT(*) AS n,\n",
        "# #        ROUND(100*COUNTIF(plan_tier IS NULL)/COUNT(*),2) AS pct_missing_plan_tier\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`\n",
        "# # GROUP BY region\n",
        "# # ORDER BY pct_missing_plan_tier DESC;"
      ],
      "id": "pUv-mDNvXHZA"
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = f\"\"\"\n",
        "SELECT\n",
        "  country,\n",
        "  COUNT(*) AS n,\n",
        "  ROUND(100 * COUNTIF(subscription_plan IS NULL) / COUNT(*), 2) AS pct_missing_subscription_plan\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.users`\n",
        "GROUP BY country\n",
        "ORDER BY pct_missing_subscription_plan DESC\n",
        "\"\"\"\n",
        "\n",
        "missing_by_country = client.query(query2).to_dataframe()\n",
        "print(\"% subscription_plan missing by country:\")\n",
        "print(missing_by_country)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CO-EU13o0BA",
        "outputId": "ea4fafa4-5ce4-4d7e-adf8-2b7d58474b86"
      },
      "id": "6CO-EU13o0BA",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% subscription_plan missing by country:\n",
            "  country      n  pct_missing_subscription_plan\n",
            "0  Canada  12384                            0.0\n",
            "1     USA  28816                            0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3FjSi7tXHZB"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that prints the three missingness percentages from (1), rounded to two decimals.\n"
      ],
      "id": "t3FjSi7tXHZB"
    },
    {
      "cell_type": "code",
      "source": [
        "query_verify = f\"\"\"\n",
        "SELECT\n",
        "  ROUND(100 * COUNTIF(country IS NULL) / COUNT(*), 2) AS pct_missing_country,\n",
        "  ROUND(100 * COUNTIF(subscription_plan IS NULL) / COUNT(*), 2) AS pct_missing_subscription_plan,\n",
        "  ROUND(100 * COUNTIF(age IS NULL) / COUNT(*), 2) AS pct_missing_age\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.users`\n",
        "\"\"\"\n",
        "\n",
        "verify_df = client.query(query_verify).to_dataframe()\n",
        "print(\"Three missingness percentages:\")\n",
        "print(verify_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJNZObI_pPW3",
        "outputId": "0cb491f0-f055-4460-c98b-6beb52a6bebe"
      },
      "id": "WJNZObI_pPW3",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Three missingness percentages:\n",
            "   pct_missing_country  pct_missing_subscription_plan  pct_missing_age\n",
            "0                  0.0                            0.0            11.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4JwQz3zXHZB"
      },
      "source": [
        "**Reflection:** Which columns are most missing? Hypothesize MCAR/MAR/MNAR and why. The most is by far age, and this makes sense since you do not have to input it, while the other ones are required."
      ],
      "id": "Y4JwQz3zXHZB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw3gvT9DXHZB"
      },
      "source": [
        "### 5.2 Duplicates (watch_history) — What & Why\n",
        "Find exact duplicate interaction records and keep **one best** per group (deterministic policy)."
      ],
      "id": "Tw3gvT9DXHZB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97K8IUrvXHZB"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Report duplicate groups on `(user_id, movie_id, event_ts, device_type)` with counts (top 20).\n",
        "2) Create table `watch_history_dedup` that keeps one row per group (prefer higher `progress_ratio`, then `minutes_watched`). Add comments.\n"
      ],
      "id": "97K8IUrvXHZB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzJwBKH5XHZB"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Detect duplicate groups (commented)\n",
        "# # SELECT user_id, movie_id, event_ts, device_type, COUNT(*) AS dup_count\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history`\n",
        "# # GROUP BY user_id, movie_id, event_ts, device_type\n",
        "# # HAVING dup_count > 1\n",
        "# # ORDER BY dup_count DESC\n",
        "# # LIMIT 20;"
      ],
      "id": "ZzJwBKH5XHZB"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client()\n",
        "\n",
        "query1 = f\"\"\"\n",
        "-- Detect duplicate groups on (user_id, movie_id, watch_date, device_type)\n",
        "SELECT\n",
        "  user_id,\n",
        "  movie_id,\n",
        "  watch_date,\n",
        "  device_type,\n",
        "  COUNT(*) AS dup_count\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history`\n",
        "GROUP BY user_id, movie_id, watch_date, device_type\n",
        "HAVING dup_count > 1\n",
        "ORDER BY dup_count DESC\n",
        "LIMIT 20\n",
        "\"\"\"\n",
        "\n",
        "duplicates_df = client.query(query1).to_dataframe()\n",
        "print(\"Top 20 duplicate groups:\")\n",
        "print(duplicates_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4G33-yjqk-t",
        "outputId": "1a13dbe5-1c1e-47d2-d634-ac09db78c3db"
      },
      "id": "L4G33-yjqk-t",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 duplicate groups:\n",
            "       user_id    movie_id  watch_date device_type  dup_count\n",
            "0   user_03310  movie_0640  2024-09-08    Smart TV         16\n",
            "1   user_00391  movie_0893  2024-08-26      Laptop         16\n",
            "2   user_03176  movie_0534  2024-01-06      Laptop         12\n",
            "3   user_03660  movie_0109  2025-05-20     Desktop         12\n",
            "4   user_09564  movie_0552  2025-01-11      Laptop         12\n",
            "5   user_08826  movie_0133  2025-04-11     Desktop         12\n",
            "6   user_04899  movie_0142  2025-01-20     Desktop         12\n",
            "7   user_02950  movie_0928  2025-06-03     Desktop         12\n",
            "8   user_09512  movie_0825  2025-01-07     Desktop         12\n",
            "9   user_02822  movie_0009  2025-08-30     Desktop         12\n",
            "10  user_03898  movie_0500  2025-07-29     Desktop         12\n",
            "11  user_06417  movie_0590  2024-01-15      Laptop         12\n",
            "12  user_09815  movie_0827  2024-05-25      Laptop         12\n",
            "13  user_06085  movie_0346  2024-03-04     Desktop         12\n",
            "14  user_03140  movie_0205  2025-09-11     Desktop         12\n",
            "15  user_01870  movie_0844  2024-06-02      Laptop         12\n",
            "16  user_02028  movie_0037  2024-08-13     Desktop         12\n",
            "17  user_00928  movie_0913  2024-01-18      Laptop         12\n",
            "18  user_05629  movie_0697  2025-01-23     Desktop         12\n",
            "19  user_01581  movie_0933  2024-03-30     Desktop         12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5u-uDGDXHZa"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Keep-one policy (commented)\n",
        "# # CREATE OR REPLACE TABLE `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` AS\n",
        "# # SELECT * EXCEPT(rk) FROM (\n",
        "# #   SELECT h.*,\n",
        "# #          ROW_NUMBER() OVER (\n",
        "# #            PARTITION BY user_id, movie_id, event_ts, device_type\n",
        "# #            ORDER BY progress_ratio DESC, minutes_watched DESC\n",
        "# #          ) AS rk\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history` h\n",
        "# # )\n",
        "# # WHERE rk = 1;"
      ],
      "id": "s5u-uDGDXHZa"
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = f\"\"\"\n",
        "-- Keep-one policy: prefer higher progress_percentage, then watch_duration_minutes\n",
        "CREATE OR REPLACE TABLE `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup` AS\n",
        "SELECT * EXCEPT(rk)\n",
        "FROM (\n",
        "  SELECT\n",
        "    h.*,\n",
        "    ROW_NUMBER() OVER (\n",
        "      PARTITION BY user_id, movie_id, watch_date, device_type\n",
        "      ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
        "    ) AS rk\n",
        "  FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history` h\n",
        ")\n",
        "WHERE rk = 1\n",
        "\"\"\"\n",
        "\n",
        "client.query(query2).result()\n",
        "print(\"✅ Table `watch_history_dedup` created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvjTPF5RrN1w",
        "outputId": "5ebb0479-0e20-411a-b305-2b358cdd16e5"
      },
      "id": "VvjTPF5RrN1w",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Table `watch_history_dedup` created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2UptOLgXHZa"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a before/after count query comparing raw vs `watch_history_dedup`.\n"
      ],
      "id": "q2UptOLgXHZa"
    },
    {
      "cell_type": "code",
      "source": [
        "query_verify = f\"\"\"\n",
        "SELECT 'raw_watch_history' AS table_name, COUNT(*) AS row_count\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history`\n",
        "UNION ALL\n",
        "SELECT 'watch_history_dedup' AS table_name, COUNT(*) AS row_count\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup`\n",
        "\"\"\"\n",
        "\n",
        "compare_df = client.query(query_verify).to_dataframe()\n",
        "print(\"Before vs after deduplication:\")\n",
        "print(compare_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDupwFzbrUP9",
        "outputId": "9163e6e4-6f9d-4f2b-80bb-a82d65db5824"
      },
      "id": "wDupwFzbrUP9",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before vs after deduplication:\n",
            "            table_name  row_count\n",
            "0  watch_history_dedup     100000\n",
            "1    raw_watch_history     420000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDBiW-0MXHZa"
      },
      "source": [
        "**Reflection:** Why do duplicates arise (natural vs system-generated)? How do they corrupt labels and KPIs? They can be someone who has two accounts or can be a bug in the system. They can have the same name and throws off all statistics."
      ],
      "id": "qDBiW-0MXHZa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j1vqzPfXHZb"
      },
      "source": [
        "### 5.3 Outliers (minutes_watched) — What & Why\n",
        "Estimate extreme values via IQR; report % outliers; **winsorize** to P01/P99 for robustness while also **flagging** extremes."
      ],
      "id": "7j1vqzPfXHZb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHbiVIKDXHZb"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Compute IQR bounds for `minutes_watched` on `watch_history_dedup` and report % outliers.\n",
        "2) Create `watch_history_robust` with `minutes_watched_capped` capped at P01/P99; return quantile summaries before/after.\n"
      ],
      "id": "hHbiVIKDXHZb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xuSxxgZXHZb"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — IQR outlier rate (commented)\n",
        "# # WITH dist AS (\n",
        "# #   SELECT\n",
        "# #     APPROX_QUANTILES(minutes_watched, 4)[OFFSET(1)] AS q1,\n",
        "# #     APPROX_QUANTILES(minutes_watched, 4)[OFFSET(3)] AS q3\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # ),\n",
        "# # bounds AS (\n",
        "# #   SELECT q1, q3, (q3-q1) AS iqr,\n",
        "# #          q1 - 1.5*(q3-q1) AS lo,\n",
        "# #          q3 + 1.5*(q3-q1) AS hi\n",
        "# #   FROM dist\n",
        "# # )\n",
        "# # SELECT\n",
        "# #   COUNTIF(h.minutes_watched < b.lo OR h.minutes_watched > b.hi) AS outliers,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(h.minutes_watched < b.lo OR h.minutes_watched > b.hi)/COUNT(*),2) AS pct_outliers\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` h\n",
        "# # CROSS JOIN bounds b;"
      ],
      "id": "-xuSxxgZXHZb"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "\n",
        "client = bigquery.Client()\n",
        "\n",
        "query1 = f\"\"\"\n",
        "-- Compute IQR bounds for watch_duration_minutes and report % outliers\n",
        "WITH dist AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(2)] AS q2,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "  FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup`\n",
        "),\n",
        "bounds AS (\n",
        "  SELECT\n",
        "    q1,\n",
        "    q3,\n",
        "    (q3 - q1) AS iqr,\n",
        "    q1 - 1.5 * (q3 - q1) AS lo,\n",
        "    q3 + 1.5 * (q3 - q1) AS hi\n",
        "  FROM dist\n",
        ")\n",
        "SELECT\n",
        "  COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) AS outliers,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100 * COUNTIF(h.watch_duration_minutes < b.lo OR h.watch_duration_minutes > b.hi) / COUNT(*), 2) AS pct_outliers\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup` h\n",
        "CROSS JOIN bounds b;\n",
        "\"\"\"\n",
        "\n",
        "iqr_df = client.query(query1).to_dataframe()\n",
        "print(\"IQR-based outlier summary:\")\n",
        "print(iqr_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Utue3Barn_Z",
        "outputId": "39e6be2f-6735-40ee-cd6a-29b5787fcc1a"
      },
      "id": "9Utue3Barn_Z",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IQR-based outlier summary:\n",
            "   outliers   total  pct_outliers\n",
            "0      3462  100000          3.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a4tsz4sXHZb"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — Winsorize + quantiles (commented)\n",
        "# # CREATE OR REPLACE TABLE `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust` AS\n",
        "# # WITH q AS (\n",
        "# #   SELECT\n",
        "# #     APPROX_QUANTILES(minutes_watched, 100)[OFFSET(1)]  AS p01,\n",
        "# #     APPROX_QUANTILES(minutes_watched, 100)[OFFSET(98)] AS p99\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # )\n",
        "# # SELECT\n",
        "# #   h.*,\n",
        "# #   GREATEST(q.p01, LEAST(q.p99, h.minutes_watched)) AS minutes_watched_capped\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup` h, q;\n",
        "# #\n",
        "# # -- Quantiles before vs after\n",
        "# # WITH before AS (\n",
        "# #   SELECT 'before' AS which, APPROX_QUANTILES(minutes_watched, 5) AS q\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_dedup`\n",
        "# # ),\n",
        "# # after AS (\n",
        "# #   SELECT 'after' AS which, APPROX_QUANTILES(minutes_watched_capped, 5) AS q\n",
        "# #   FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust`\n",
        "# # )\n",
        "# # SELECT * FROM before UNION ALL SELECT * FROM after;"
      ],
      "id": "6a4tsz4sXHZb"
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = f\"\"\"\n",
        "-- Create robust version of watch_history_dedup with capped watch_duration_minutes\n",
        "CREATE OR REPLACE TABLE `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_robust` AS\n",
        "WITH q AS (\n",
        "  SELECT\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)] AS p01,\n",
        "    APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(99)] AS p99\n",
        "  FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup`\n",
        ")\n",
        "SELECT\n",
        "  h.*,\n",
        "  GREATEST(q.p01, LEAST(q.p99, h.watch_duration_minutes)) AS watch_duration_capped\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup` h, q;\n",
        "\"\"\"\n",
        "\n",
        "client.query(query2).result()\n",
        "print(\"✅ Table `watch_history_robust` created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsXt78icrteA",
        "outputId": "c913f75a-df14-4b3c-86c7-a46acda5a237"
      },
      "id": "JsXt78icrteA",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Table `watch_history_robust` created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2_DWf8fXHZb"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that shows min/median/max before vs after capping.\n"
      ],
      "id": "v2_DWf8fXHZb"
    },
    {
      "cell_type": "code",
      "source": [
        "query_verify = f\"\"\"\n",
        "SELECT 'before' AS which,\n",
        "  MIN(watch_duration_minutes) AS min_val,\n",
        "  MAX(watch_duration_minutes) AS max_val\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_dedup`\n",
        "UNION ALL\n",
        "SELECT 'after' AS which,\n",
        "  MIN(watch_duration_capped) AS min_val,\n",
        "  MAX(watch_duration_capped) AS max_val\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_robust`;\n",
        "\"\"\"\n",
        "\n",
        "verify_df = client.query(query_verify).to_dataframe()\n",
        "print(\"Min/Max before vs after capping:\")\n",
        "print(verify_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGwal2KWrzgE",
        "outputId": "f69382fa-264c-48aa-f001-601c3b441858"
      },
      "id": "kGwal2KWrzgE",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Min/Max before vs after capping:\n",
            "    which  min_val  max_val\n",
            "0  before      0.2    799.3\n",
            "1   after      4.4    366.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20EPYS5oXHZc"
      },
      "source": [
        "**Reflection:** When might capping be harmful? Name a model type less sensitive to outliers and why. It can be harmful if it gets rid of values but are not actually outliers, thus getting rid of useful information."
      ],
      "id": "20EPYS5oXHZc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpmEHFy_XHZc"
      },
      "source": [
        "### 5.4 Business anomaly flags — What & Why\n",
        "Human-readable flags help both product decisioning and ML features (e.g., binge behavior)."
      ],
      "id": "DpmEHFy_XHZc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4NC18DWXHZc"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **three BigQuery SQL cells** (adjust if columns differ):\n",
        "1) In `watch_history_robust`, compute and summarize `flag_binge` for sessions > 8 hours.\n",
        "2) In `users`, compute and summarize `flag_age_extreme` if age can be parsed from `age_band` (<10 or >100).\n",
        "3) In `movies`, compute and summarize `flag_duration_anomaly` where `duration_min` < 15 or > 480 (if exists).\n",
        "Each cell should output count and percentage and include 1–2 comments.\n"
      ],
      "id": "i4NC18DWXHZc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1OgnW0zXHZc"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_binge (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(minutes_watched > 8*60) AS sessions_over_8h,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(minutes_watched > 8*60)/COUNT(*),2) AS pct\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.watch_history_robust`;"
      ],
      "id": "W1OgnW0zXHZc"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import os\n",
        "client = bigquery.Client()\n",
        "\n",
        "query1 = f\"\"\"\n",
        "-- Flag sessions over 8 hours\n",
        "SELECT\n",
        "  COUNTIF(watch_duration_minutes > 8*60) AS sessions_over_8h,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100 * COUNTIF(watch_duration_minutes > 8*60) / COUNT(*), 2) AS pct_over_8h\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_robust`;\n",
        "\"\"\"\n",
        "\n",
        "flag_binge = client.query(query1).to_dataframe()\n",
        "print(\"Flag: Binge sessions (>8h):\")\n",
        "print(flag_binge)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVH-Tg4WsL5A",
        "outputId": "46cea535-e6b2-462f-fcc1-e7a2fb41cf17"
      },
      "id": "eVH-Tg4WsL5A",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flag: Binge sessions (>8h):\n",
            "   sessions_over_8h   total  pct_over_8h\n",
            "0               639  100000         0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjNPceEOXHZc"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_age_extreme (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) < 10 OR\n",
        "# #           CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) > 100) AS extreme_age_rows,\n",
        "# #   COUNT(*) AS total,\n",
        "# #   ROUND(100*COUNTIF(CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) < 10 OR\n",
        "# #                     CAST(REGEXP_EXTRACT(age_band, r'\\d+') AS INT64) > 100)/COUNT(*),2) AS pct\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.users`;"
      ],
      "id": "OjNPceEOXHZc"
    },
    {
      "cell_type": "code",
      "source": [
        "query2 = f\"\"\"\n",
        "-- Flag extreme ages (<10 or >100)\n",
        "SELECT\n",
        "  COUNTIF(age < 10 OR age > 100) AS extreme_age_rows,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100 * COUNTIF(age < 10 OR age > 100) / COUNT(*), 2) AS pct_extreme_age\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.users`;\n",
        "\"\"\"\n",
        "\n",
        "flag_age = client.query(query2).to_dataframe()\n",
        "print(\"Flag: Extreme age values (<10 or >100):\")\n",
        "print(flag_age)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOMu60rZsPRX",
        "outputId": "395921e3-72ea-4d19-c832-29f01dda0bd3"
      },
      "id": "uOMu60rZsPRX",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flag: Extreme age values (<10 or >100):\n",
            "   extreme_age_rows  total  pct_extreme_age\n",
            "0               716  41200             1.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcQA1RMyXHZd"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# # EXAMPLE (from LLM) — flag_duration_anomaly (commented)\n",
        "# # SELECT\n",
        "# #   COUNTIF(duration_min < 15) AS titles_under_15m,\n",
        "# #   COUNTIF(duration_min > 8*60) AS titles_over_8h,\n",
        "# #   COUNT(*) AS total\n",
        "# # FROM `${GOOGLE_CLOUD_PROJECT}.netflix.movies`;"
      ],
      "id": "QcQA1RMyXHZd"
    },
    {
      "cell_type": "code",
      "source": [
        "query3 = f\"\"\"\n",
        "-- Flag duration anomalies (<15 or >480 minutes)\n",
        "SELECT\n",
        "  COUNTIF(duration_minutes < 15 OR duration_minutes > 480) AS duration_anomalies,\n",
        "  COUNT(*) AS total,\n",
        "  ROUND(100 * COUNTIF(duration_minutes < 15 OR duration_minutes > 480) / COUNT(*), 2) AS pct_anomalies\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.movies`;\n",
        "\"\"\"\n",
        "\n",
        "flag_duration = client.query(query3).to_dataframe()\n",
        "print(\"Flag: Duration anomalies (<15 or >480 min):\")\n",
        "print(flag_duration)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlF1SWcisSlq",
        "outputId": "825f1650-353b-4015-e06c-ab7b5099ee10"
      },
      "id": "IlF1SWcisSlq",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flag: Duration anomalies (<15 or >480 min):\n",
            "   duration_anomalies  total  pct_anomalies\n",
            "0                  92   4160           2.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAMZKFqiXHZd"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single compact summary query that returns two columns per flag: `flag_name, pct_of_rows`.\n"
      ],
      "id": "KAMZKFqiXHZd"
    },
    {
      "cell_type": "code",
      "source": [
        "query_verify = f\"\"\"\n",
        "SELECT 'flag_binge' AS flag_name,\n",
        "       ROUND(100 * COUNTIF(watch_duration_minutes > 8*60) / COUNT(*), 2) AS pct_of_rows\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.watch_history_robust`\n",
        "UNION ALL\n",
        "SELECT 'flag_age_extreme',\n",
        "       ROUND(100 * COUNTIF(age < 10 OR age > 100) / COUNT(*), 2)\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.users`\n",
        "UNION ALL\n",
        "SELECT 'flag_duration_anomaly',\n",
        "       ROUND(100 * COUNTIF(duration_minutes < 15 OR duration_minutes > 480) / COUNT(*), 2)\n",
        "FROM `{os.environ['GOOGLE_CLOUD_PROJECT']}.netflix.movies`;\n",
        "\"\"\"\n",
        "\n",
        "summary_flags = client.query(query_verify).to_dataframe()\n",
        "print(\"Compact flag summary:\")\n",
        "print(summary_flags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D9xIlkisfvq",
        "outputId": "538c80e6-051f-431d-c018-81b5eb272c5c"
      },
      "id": "_D9xIlkisfvq",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compact flag summary:\n",
            "               flag_name  pct_of_rows\n",
            "0             flag_binge         0.64\n",
            "1       flag_age_extreme         1.74\n",
            "2  flag_duration_anomaly         2.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzbGgefQXHZd"
      },
      "source": [
        "**Reflection:** Which anomaly flag is most common? Which would you keep as a feature and why? the duration anomaly is the most common."
      ],
      "id": "lzbGgefQXHZd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL4biF0VXHZe"
      },
      "source": [
        "## 6) Save & submit — What & Why\n",
        "Reproducibility: save artifacts and document decisions so others can rerun and audit."
      ],
      "id": "oL4biF0VXHZe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFbGQG4fXHZe"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a checklist (Markdown) students can paste at the end:\n",
        "- Save this notebook to the team Drive.\n",
        "- Export a `.sql` file with your DQ queries and save to repo.\n",
        "- Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts.\n"
      ],
      "id": "VFbGQG4fXHZe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSuEkkbkXHZe"
      },
      "source": [
        "## Grading rubric (quick)\n",
        "- Profiling completeness (30)  \n",
        "- Cleaning policy correctness & reproducibility (40)  \n",
        "- Reflection/insight (20)  \n",
        "- Hygiene (naming, verification, idempotence) (10)\n"
      ],
      "id": "TSuEkkbkXHZe"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}